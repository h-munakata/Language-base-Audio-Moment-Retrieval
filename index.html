<!doctype html>
<html>
    <head>
        <!-- BOOTSTRAP CORE STYLE  -->
        <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/css/bootstrap.min.css" integrity="sha384-9aIt2nRpC12Uk9gS9baDl411NQApFmC26EwAOH8WgZl5MYYxFfc+NcPb1dKGj7Sk" crossorigin="anonymous">
        <!-- GOOGLE FONT -->
        <link href='https://fonts.googleapis.com/css?family=Open+Sans' rel='stylesheet' type='text/css' crossorigin="anonymous" />

        <!-- BOOTSTRAP JS -->
        <script src="https://code.jquery.com/jquery-3.5.1.slim.min.js" integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj" crossorigin="anonymous"></script>
        <script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js" integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo" crossorigin="anonymous"></script>
        <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/js/bootstrap.min.js" integrity="sha384-OgVRvuATP1z7JjHLkuOU7Xw704+h835Lr+6QL9UvYjZE3Ipu6Tp75j7Bh/kR0JKI" crossorigin="anonymous"></script>
    
        <!-- CUSTOM STYLE CSS -->

        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />

        <!-- Page description -->   


        <!-- Page keywords -->
        

        <!-- Page author -->
        
        <meta name="author" content="Hokuto Munakata" />
        

        <!-- Site title -->
        
        <title>Language-based Audio Moment Retrieval - Hokuto Munakata</title>
    </head>
    <body>


    <section class="jumbotron text-center">
        <div class="container">
            
                <h1 class="jumbotron-heading">Language-based Audio Moment Retrieval</h1>
            
            
                <p class="lead">
                    Hokuto Munakata, Taichi Nishimura, Shota Nakada, Tatsuya Komatsu
                </p>
                <p class="lead">
                    LY Corporation, Japan<br>

                </p>
                <p class="lead">
                    Under review<br>
                    [<a href="https://arxiv.org/abs/2409.15672">Paper</a>]
                    [<a href="https://zenodo.org/records/13806234">CLAP Features of Clotho-moment</a>]
                    <br>
                    [<a href="https://github.com/line/lighthouse">Code (AMR)</a>]
                    [<a href="https://github.com/h-munakata/Lighthouse-Wrapper-for-Audio-Moment-Retrieval">Code (Data preparation & Zero-shot SED, under construction)</a>]
                </p>
                


            
        </div>
    </section>
            
    <p><link rel="stylesheet" type="text/css" href="style.css"></p>

<div class="container">
    <div class="row"><h2>Abstract</h2></div>
    <div class="row">
        <p class="lead">
            In this paper, we propose and design a new task called audio moment retrieval (AMR).
            Unlike conventional language-based audio retrieval tasks that search for short audio clips from an audio database, AMR aims to predict relevant moments in untrimmed long audio based on a text query.
            Given the lack of prior work in AMR, we first build a dedicated dataset, Clotho-Moment, consisting of large-scale simulated audio recordings with moment annotations.
            We then propose a DETR-based model, named Audio Moment DETR (AM-DETR), as a fundamental framework for AMR tasks.
            This model captures temporal dependencies within audio features, inspired by similar video moment retrieval tasks, thus surpassing conventional clip-level audio retrieval methods.
            Additionally, we provide manually annotated datasets to properly measure the effectiveness and robustness of our methods on real data.
            Experimental results show that AM-DETR, trained with Clotho-Moment, outperforms a baseline model that applies a clip-level audio retrieval method with a sliding window on all metrics, particularly improving Recall1@0.7 by 9.00 points.
            Our datasets and code are publicly available in https://h-munakata.github.io/Language-based-Audio-Moment-Retrieval
        </p>
    </div>
    <div align="center">
        <img src="images/task_overview.png" width="50%">
        <figcaption> Fig. 1.
            An overview of audio moment retrieval (AMR). Given a long audioand text query, 
            <br> the system retrieves the relevant moments as the start and end
            time stamps.
        </figcaption>
    </div>

    <div class="row"><h2>Clotho-Moment</h2></div>
    <div class="row">
        <p class="lead">
            We construct datasets to retrieve audio moments relevant to the text query from untrimmed audio data.
            The audio we assume for AMR is one minute long and includes some specific scenes that can be represented by text such as audio of a sports broadcast including a scene in which the goal is scored and people cheering.
            Based on these assumptions, we create a large-scale simulated AMR dataset named <b>Clotho-Moment</b>.
            As shown in Figure 2, this dataset is generated with Clotho [1] and Walking tour [2] as foreground and background, respectively.
        </p>
    </div>
    <div align="center">
        <img src="images/clotho-moment.png" width="30%">
        <figcaption>Fig. 2.
            Clotho-Moment containing audio and text query-audio moment pairs is generated by overlaying Clotho on Walking Tour.
        </figcaption>
    </div>
    <br>

    <div class="row"><h2>Audio moment DETR (AM-DETR)</h2></div>
    <div class="row">
        <p class="lead">
            We propose <b>Audio Moment DETR (AM-DETR)</b>, an AMR model inspired by the model proposed for video moment retrieval (VMR) [3-5].
            As shown in Figure 3, AM-DETR consists of a feature extractor and a DETR-based network the same as the VMR models.
            The key part of AM-DETR is the DETR-based network [3-5] that captures the temporal dependency between audio features.
            Specifically, the architecture of AM-DETR is based on QD-DETR, which is simple but achieves high retrieval performance for VMR by capturing both the temporal dependency between video frames and cross-modal dependency between frames and text query [4].


        </p>
    </div>
    <div align="center">
        <img src="images/am-detr.png" width="70%">
        <figcaption>Fig. 3.
            The architecture of Audio Moment DETR. This model first extracts embedding by audio/text encoder and then the DETR-based network transforms the embedding to pairs of the audio moment and its confidence score.
        </figcaption>
    </div>
    <br>

</div>

<div class="container">
        <div class="row"><h2>Sample data & predicted moments</h2>
            <p class="lead">
                We demonstrate sample data of Clotho-Moment and UnAV-100 subset, and the predicted moments by AM-DETR trained with Clotho-Moment.
                AM-DETR is trained using the open-source library called Lighthouse [6].
        </div>
        <h4>Clotho-Moment</h4>
        <table   style="table-layout: fixed;">
            <thead>
                <tr>
                    <th style="text-align: center; width: 50%;">Audio sample</th>
                    <th style="text-align: center; width: 30%;">Text Query</th>
                    <th style="text-align: center; width: 10%;">Ground-truth<br>moment</th>
                    <th style="text-align: center; width: 10%;">Predicted<br>moment</th>
                </tr>
            </thead>
            <tbody>


            <tr>
                <td >
                    <audio id="audioSample1" controls preload="metadata" src="demo/Istanbul_0.0_60.0.wav">
                    </audio>
                </td >
                <td >
                    <p>A woman is making an announcement <br> about something</p>
                </td >
                <td >
                    <button onclick="playAudio('audioSample1', 10)">[11s, 33s]</button>
                </td>
                <td >
                    <button onclick="playAudio('audioSample1', 10)">[11s, 33s]</button>
                </td>
            </tr>

            <tr></tr>
                <td >
                    <audio id="audioSample2" controls preload="metadata" src="demo/Istanbul_1.0_61.0.wav">
                    </audio>
                </td >
                <td >
                    <p>Bird are singing nearby a source of water <br> such as a small waterfall.</p>
                </td >
                <td >
                    <button onclick="playAudio('audioSample2', 19)">[20s, 48s]</button>
                </td>
                <td >
                    <button onclick="playAudio('audioSample2', 18)">[19s, 47s]</button>
                </td>
            </tr>

            <tr></tr>
                <td >
                    <audio id="audioSample3" controls preload="metadata" src="demo/Istanbul_4.0_64.0.wav">
                    </audio>
                </td >
                <td >
                    <p>The fast cars pass by as the rain falls.</p>
                </td >
                <td >
                    <button onclick="playAudio('audioSample3', 0)">[0s, 9s]</button>
                </td>
                <td >
                    <button onclick="playAudio('audioSample3', 0)">[1s, 17s]</button>
                </td>
            </tr>

            <tr></tr>
                <td >
                    <audio id="audioSample4" controls preload="metadata" src="demo/Istanbul_5.0_65.0.wav">
                    </audio>
                </td >
                <td >
                    <p>A plane accelerates and takes off <br> before fading in the distance.</p>
                </td >
                <td >
                    <button onclick="playAudio('audioSample4', 32)">[33s, 54s]</button>
                </td>
                <td >
                    <button onclick="playAudio('audioSample4', 31)">[32s, 53s]</button>
                </td>
            </tr>

            <tr></tr>
                <td >
                    <audio id="audioSample5" controls preload="metadata" src="demo/Istanbul_8.0_68.0.wav">
                    </audio>
                </td >
                <td >
                    <p>A burst of metal instruments playing is <br> followed by clapping, people talking and <br> more rhythmic sounds of horn music.</p>
                </td >
                <td >
                    <button onclick="playAudio('audioSample5', 15)">[16s, 31s]</button>
                </td>
                <td >
                    <button onclick="playAudio('audioSample5', 13)">[14s, 31s]</button>
                </td>
            </tr>

            <tr></tr>
                <td >
                    <audio id="audioSample6" controls preload="metadata" src="demo/Istanbul_8.0_68.0.wav">
                    </audio>
                </td >
                <td >
                    <p>A sink of water is being used to wash up in.</p>
                </td >
                <td >
                    <button onclick="playAudio('audioSample6', 37)">[38s, 55s]</button>
                </td>
                <td >
                    <button onclick="playAudio('audioSample6', 36)">[37s, 53s]</button>
                </td>
            </tr>

            </tbody>
        </table>

        <p><br></p>
        <h4>UnAV-100 subset</h4>
        </div>

        <table   style="table-layout: fixed;">
            <thead>
                <tr>
                    <th style="text-align: center; width: 50%;">Audio sample</th>
                    <th style="text-align: center; width: 30%;">Text Query</th>
                    <th style="text-align: center; width: 10%;">Ground-truth<br>moment</th>
                    <th style="text-align: center; width: 10%;">Predicted<br>moment</th>
                </tr>
            </thead>
            <tbody>


            <tr>
                <td >
                    <audio id="audioSample7" controls preload="metadata" src="demo/TtO9wzZ3LFA.wav">
                    </audio>
                </td >
                <td >
                    <p>Someone dries his hair with a hair dryer.</p>
                </td >
                <td >
                    <button onclick="playAudio('audioSample7', 18)">[19s, 34s]</button>
                </td>
                <td >
                    <button onclick="playAudio('audioSample7', 18)">[19s, 34s]</button>
                </td>
            </tr>

            <tr></tr>
                <td >
                    <audio id="audioSample8" controls preload="metadata" src="demo/2PHV2xNjGVU.wav">
                    </audio>
                </td >
                <td >
                    <p>Two people perform tap dance to background music</p>
                </td >
                <td >
                    <button onclick="playAudio('audioSample8', 9)">[10s, 37s]</button>
                </td>
                <td >
                    <button onclick="playAudio('audioSample8', 13)">[14s, 34s]</button>
                </td>
            </tr>

            <tr></tr>
                <td >
                    <audio id="audioSample9" controls preload="metadata" src="demo/1a-ODBWMUAE.wav">
                    </audio>
                </td >
                <td >
                    <p>Water cascades down from a waterfall.</p>
                </td >
                <td >
                    <button onclick="playAudio('audioSample9', 4)">[5s, 26s]</button>
                </td>
                <td >
                    <button onclick="playAudio('audioSample9', 5)">[6s, 26s]</button>
                </td>
            </tr>

            <tr></tr>
                <td >
                    <audio id="audioSample10" controls preload="metadata" src="demo/0T9__Qqp2Go.wav">
                    </audio>
                </td >
                <td >
                    <p>Spectators watch sports and cheer.</p>
                </td >
                <td >
                    <button onclick="playAudio('audioSample10', 17)">[18s, 41s]</button>
                </td>
                <td >
                    <button onclick="playAudio('audioSample10', 13)">[14s, 36s]</button>
                </td>
            </tr>

            <tr></tr>
                <td >
                    <audio id="audioSample11" controls preload="metadata" src="demo/-2FKhYSw_zU.wav">
                    </audio>
                </td >
                <td >
                    <p>Men and women sing in a chorus in a hall.</p>
                </td >
                <td >
                    <button onclick="playAudio('audioSample11', 27)">[28s, 46s]</button>
                </td>
                <td >
                    <button onclick="playAudio('audioSample11', 27)">[28s, 41s]</button>
                </td>
            </tr>

            <tr></tr>
                <td >
                    <audio id="audioSample12" controls preload="metadata" src="demo/sNicRCa4sTI.wav">
                    </audio>
                </td >
                <td >
                    <p>A sports car starts with a loud noise.</p>
                </td >
                <td >
                    <button onclick="playAudio('audioSample12', 34)">[35s, 45s]</button>
                </td>
                <td >
                    <button onclick="playAudio('audioSample12', 1)">[1s, 14s]</button>
                </td>
            </tr>

            </tbody>
        </table>
    <br>
</div>

<div class="container">
    <h1>References</h1>
    <div>
        [1] K. Drossos, S. Lipping, and T. Virtanen, “Clotho: An audio captioning dataset,” in Proc. ICASSP, 2020, pp. 736–740.
        <br>
        [2]  S. Venkataramanan, M. N. Rizve, J. Carreira, Y. M. Asano, and Y. Avrithis, “Is imagenet worth 1 video? learning strong image encoders from 1 long unlabelled video,” in Proc. ICLR, 2024.
        <br>
        [3] J. Lei, T. L. Berg, and M. Bansal, “Detecting moments and highlights in videos via natural language queries,” in Proc. NeurIPS, vol. 34, 2021, pp. 11 846–11 858.
        <br>
        [4] W. Moon, S. Hyun, S. Park, D. Park, and J.-P. Heo, “Query-dependent video representation for moment retrieval and highlight detection,” in Proc. CVPR, 2023, pp. 23023–23033.
        <br>
        [5] W. Moon, S. Hyun, S. Lee, and J.-P. Heo, “Correlation-guided query-dependency calibration in video representation learning for temporal grounding,” arXiv preprint arXiv:2311.08835, 2023.
        <br>
        [6] T. Nishimura, S. Nakada, H. Munakata, and T. Komatsu, “Lighthouse: A user-friendly library for reproducible video moment retrieval and highlight detection,” arXiv preprint arXiv:2408.02901, 2024.
    </div>
</div>

<script>
    function playAudio(id, time) {
        // stop all other audio
        var allAudio = document.getElementsByTagName('audio');
        for (var i = 0; i < allAudio.length; i++) {
            if (allAudio[i] != audio) {
                allAudio[i].pause();
            }
        }

        // Play the audio
        var audio = document.getElementById(id);
        audio.currentTime = time;
        audio.play();
    }
</script>
        
    </body>
</html>
